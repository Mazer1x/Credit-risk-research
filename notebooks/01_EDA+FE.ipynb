{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83430079-b924-4472-bafe-9d8e19d56e8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploratory Data Analysis & Feature Engineering\n",
    "\n",
    "## Introduction\n",
    "This notebook focuses on exploratory data analysis (EDA) and feature engineering for the given dataset.  \n",
    "The goal is to understand the data structure, identify key patterns and issues, and prepare a clean, well-structured feature set suitable for downstream modeling.\n",
    "\n",
    "The notebook does **not** include model training or evaluation. All steps are limited to data inspection, preprocessing, and feature construction.\n",
    "\n",
    "## Scope\n",
    "The analysis includes:\n",
    "- Initial data inspection and validation\n",
    "- Handling missing values and inconsistencies\n",
    "- Feature transformation and creation\n",
    "- Preparation of a modeling-ready dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28cde236-8b30-47c5-828c-60b45c4b7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21935293-8bd3-4ae9-8969-322a713c60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = kagglehub.dataset_download(\"ranadeep/credit-risk-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a47ee7-b4f8-4409-9549-3a09153a9a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/21 22:08:30 WARN Utils: Your hostname, mazer1x-v15xv17xrnx, resolves to a loopback address: 127.0.1.1; using 192.168.0.107 instead (on interface wlp0s20f3)\n",
      "25/12/21 22:08:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/21 22:08:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# We'll use Spark, our file is too big for Pandas\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName('Spark')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b310dd1d-8292-41a8-a76c-c3e18d3a18a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mazer1x/miniconda3/envs/menv/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../data/loan/loan.csv',header=True) # our main dataset\n",
    "columns_desc_pd = pd.read_excel('../data/LCDataDictionary.xlsx')\n",
    "columns_describe = spark.createDataFrame(columns_desc_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90be8fb0-0272-4eb6-8bf6-d2ec4d79c725",
   "metadata": {},
   "source": [
    "## EDA | NULL and NaN fill/drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bdd645-794c-4c64-b325-d612a563a7ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Checking Column Types, Missing Values, and Uniqueness\n",
    "\n",
    "In this step, we examine the data types of each column, assess the proportion of missing values (NaN), and analyze the sparsity of unique values to understand the distribution and variability in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ee41b8-8be0-48f7-97e8-d33df9af5289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'string': 74}\n"
     ]
    }
   ],
   "source": [
    "# Let's check the column types\n",
    "columns = {}\n",
    "for _,t in df.dtypes:\n",
    "    if t not in columns: columns[t]=0\n",
    "    columns[t]+=1\n",
    "pprint(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de283243-f662-4c57-ac46-972af263b733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# And let's look at % NaNs\n",
    "nan_percentage = df.select([\n",
    "    (F.round(\n",
    "        F.sum(\n",
    "            (F.col(c).isNull() | (F.col(c) == \"NaN\") | (F.col(c) == \"\")).cast(\"int\")\n",
    "        ) / df.count(),\n",
    "        2\n",
    "    )* 100).alias(c)\n",
    "    for c in df.columns\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00a2ba0c-bb13-4a85-aad4-e845c18e5d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/21 22:09:11 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "rows_dict = (nan_percentage.collect()[0]).asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab269c4-2a28-4b7b-9525-81f2ecd7e782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desc 52.0\n",
      "mths_since_last_delinq 57.99999999999999\n",
      "mths_since_last_record 89.0\n",
      "next_pymnt_d 66.0\n",
      "mths_since_last_major_derog 88.0\n",
      "annual_inc_joint 100.0\n",
      "dti_joint 100.0\n",
      "verification_status_joint 100.0\n",
      "tot_coll_amt 47.0\n",
      "tot_cur_bal 47.0\n",
      "open_acc_6m 100.0\n",
      "open_il_6m 100.0\n",
      "open_il_12m 100.0\n",
      "open_il_24m 100.0\n",
      "mths_since_rcnt_il 100.0\n",
      "total_bal_il 100.0\n",
      "il_util 100.0\n",
      "open_rv_12m 100.0\n",
      "open_rv_24m 100.0\n",
      "max_bal_bc 100.0\n",
      "all_util 100.0\n",
      "total_rev_hi_lim 47.0\n",
      "inq_fi 100.0\n",
      "total_cu_tl 100.0\n",
      "inq_last_12m 100.0\n"
     ]
    }
   ],
   "source": [
    "# Output columns with % NaNs > 10\n",
    "c = 0\n",
    "columns_to_drop = []\n",
    "for k,v in rows_dict.items(): \n",
    "    if v > 10: \n",
    "        c+=1\n",
    "        columns_to_drop.append(k)\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb2c962-fc14-4917-97b4-59a3f29c0365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 25\n"
     ]
    }
   ],
   "source": [
    "print(len(df.columns),c) # Drop columns due to large % of NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b8ee30a-5e0e-4758-947a-7b855a938e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3187e730-7659-4d47-9fb5-f27e7b9c2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will drop columns with a large % of unique values, this will not help in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e646df0d-4c5c-4230-bc74-bd0aa7442fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 91113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member_id 91113\n",
      "loan_amnt 1095\n",
      "funded_amnt 1155\n",
      "funded_amnt_inv 9373\n",
      "term 2\n",
      "int_rate 448\n",
      "installment 25056\n",
      "grade 8\n",
      "sub_grade 36\n",
      "emp_title 53610\n",
      "emp_length 13\n",
      "home_ownership 6\n",
      "annual_inc 9282\n",
      "verification_status 4\n",
      "issue_d 60\n",
      "loan_status 10\n",
      "pymnt_plan 3\n",
      "url 91113\n",
      "purpose 268\n",
      "title 32200\n",
      "zip_code 1076\n",
      "addr_state 269\n",
      "dti 3697\n",
      "delinq_2yrs 228\n",
      "earliest_cr_line 758\n",
      "inq_last_6mths 219\n",
      "open_acc 144\n",
      "pub_rec 110\n",
      "revol_bal 34069\n",
      "revol_util 1257\n",
      "total_acc 226\n",
      "initial_list_status 157\n",
      "out_prncp 25128\n",
      "out_prncp_inv 25330\n",
      "total_pymnt 86617\n",
      "total_pymnt_inv 86075\n",
      "total_rec_prncp 37763\n",
      "total_rec_int 80117\n",
      "total_rec_late_fee 2915\n",
      "recoveries 6805\n",
      "collection_recovery_fee 5129\n",
      "last_pymnt_d 188\n",
      "last_pymnt_amnt 60930\n",
      "last_credit_pull_d 208\n",
      "collections_12_mths_ex_med 149\n",
      "policy_code 77\n",
      "application_type 55\n",
      "acc_now_delinq 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91113"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i,df.select(i).distinct().count())\n",
    "df.select(\"*\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7145e-315a-4d61-a22c-1411f72c4a1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Columns analysis and casting to 'double'\n",
    "**Columns marked with `?` require extra verification before dropping:**  \n",
    "`id`, `member_id`, `emp_title`, `url`, `title(?)`, `zip_code`, `delinq_2yrs(?)`, `inq_last_6mths(?)`, `open_acc(?)`, `initial_list_status(?)`, `issue_d`, `pymnt_plan`\n",
    "\n",
    "**Post-issue columns** *(contain information generated after loan issuance and should be removed to avoid data leakage):*  \n",
    "`out_prncp`, `out_prncp_inv`, `total_pymnt`, `total_pymnt_inv`, `total_rec_prncp`, `total_rec_int`, `total_rec_late_fee`,  \n",
    "`recoveries`, `collection_recovery_fee`, `last_pymnt_d`, `last_pymnt_amnt`, `collections_12_mths_ex_med`, `last_credit_pull_d`\n",
    "\n",
    "**Note:** We will start by dropping the post-issue columns first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "475c2bec-f84d-49b7-9a99-afce0ebdd890",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_issue_columns = ['out_prncp',\n",
    "    'out_prncp_inv',\n",
    "    'total_pymnt',\n",
    "    'total_pymnt_inv',\n",
    "    'total_rec_prncp',\n",
    "    'total_rec_int',\n",
    "    'total_rec_late_fee',\n",
    "    'recoveries',\n",
    "    'collection_recovery_fee',\n",
    "    'last_pymnt_d',\n",
    "    'last_pymnt_amnt',\n",
    "    'collections_12_mths_ex_med',\n",
    "    'last_credit_pull_d'\n",
    "] + [\n",
    "    'id',\n",
    "    'member_id',\n",
    "    'emp_title',\n",
    "    'url',\n",
    "    'zip_code',\n",
    "    'issue_d',\n",
    "    'pymnt_plan'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0202c9b7-c51b-40b8-94f4-7d17c3581804",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*post_issue_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44185598-d3f4-41f3-ae98-58251ecea678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of \"problematic\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "086ab060-8158-4241-b391-9998225b521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_cast = ['delinq_2yrs','inq_last_6mths','open_acc','initial_list_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "393e56ae-7f6c-4d2a-993d-1b7bd9d4d642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------+-------------------+\n",
      "|delinq_2yrs|inq_last_6mths|open_acc|initial_list_status|\n",
      "+-----------+--------------+--------+-------------------+\n",
      "|         31|            31|     111|              90904|\n",
      "+-----------+--------------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in columns_to_cast:\n",
    "    df1 = df.withColumn(i, F.col(i).try_cast('double'))\n",
    "df1.select([\n",
    "    F.count(\n",
    "        F.when(\n",
    "            F.col(c).isNull(), 1)\n",
    "    ).alias(c)\n",
    "    for c in columns_to_cast\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b426c33e-3285-4ec5-8339-5366763b9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, we can drop initial_list_status and remove columns with NULL-s: delinq_2yrs,inq_last_6mths,open_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1222461-23bb-4bfc-ab98-a2f968ae40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('initial_list_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00d69173-053a-42c3-b1a6-46d8432a1620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 526:========================>                               (6 + 8) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " loan_amnt           | 0   \n",
      " funded_amnt         | 0   \n",
      " funded_amnt_inv     | 0   \n",
      " term                | 0   \n",
      " int_rate            | 0   \n",
      " installment         | 0   \n",
      " grade               | 0   \n",
      " sub_grade           | 0   \n",
      " emp_length          | 0   \n",
      " home_ownership      | 0   \n",
      " annual_inc          | 0   \n",
      " verification_status | 0   \n",
      " loan_status         | 0   \n",
      " purpose             | 0   \n",
      " title               | 0   \n",
      " addr_state          | 0   \n",
      " dti                 | 0   \n",
      " delinq_2yrs         | 0   \n",
      " earliest_cr_line    | 0   \n",
      " inq_last_6mths      | 0   \n",
      " open_acc            | 0   \n",
      " pub_rec             | 0   \n",
      " revol_bal           | 0   \n",
      " revol_util          | 0   \n",
      " total_acc           | 0   \n",
      " policy_code         | 0   \n",
      " application_type    | 0   \n",
      " acc_now_delinq      | 0   \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = df.dropna() # data with NULL // NaN is less than 1% of the data\n",
    "df.select([\n",
    "    F.count(\n",
    "        F.when(\n",
    "            F.col(c).isNull(), 1)\n",
    "    ).alias(c)\n",
    "    for c in df.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ed17d96-4fa3-42de-bb4d-b1cd705af963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's convert the data into the formats required for GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cea2ba1f-132a-4518-92f9-c4af1b4bfd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " loan_amnt           | 5000.0      \n",
      " funded_amnt         | 5000.0      \n",
      " funded_amnt_inv     | 4975.0      \n",
      " term                |  36 months  \n",
      " int_rate            | 10.65       \n",
      " installment         | 162.87      \n",
      " grade               | B           \n",
      " sub_grade           | B2          \n",
      " emp_length          | 10+ years   \n",
      " home_ownership      | RENT        \n",
      " annual_inc          | 24000.0     \n",
      " verification_status | Verified    \n",
      " loan_status         | Fully Paid  \n",
      " purpose             | credit_card \n",
      " title               | Computer    \n",
      " addr_state          | AZ          \n",
      " dti                 | 27.65       \n",
      " delinq_2yrs         | 0.0         \n",
      " earliest_cr_line    | Jan-1985    \n",
      " inq_last_6mths      | 1.0         \n",
      " open_acc            | 3.0         \n",
      " pub_rec             | 0.0         \n",
      " revol_bal           | 13648.0     \n",
      " revol_util          | 83.7        \n",
      " total_acc           | 9.0         \n",
      " policy_code         | 1.0         \n",
      " application_type    | INDIVIDUAL  \n",
      " acc_now_delinq      | 0.0         \n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "df.show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b1843c8-b5ba-43a5-a823-7daa183c7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_cols = ['loan_amnt','funded_amnt','funded_amnt_inv','int_rate','installment','annual_inc','dti','delinq_2yrs',\n",
    "           'inq_last_6mths','open_acc','pub_rec','revol_bal','revol_util','total_acc','policy_code','acc_now_delinq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56448337-dcef-4d17-9e0b-5922c6bcf313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 530:===========================================>           (11 + 3) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------\n",
      " loan_amnt       | 0   \n",
      " funded_amnt     | 0   \n",
      " funded_amnt_inv | 0   \n",
      " int_rate        | 0   \n",
      " installment     | 0   \n",
      " annual_inc      | 0   \n",
      " dti             | 54  \n",
      " delinq_2yrs     | 54  \n",
      " inq_last_6mths  | 54  \n",
      " open_acc        | 52  \n",
      " pub_rec         | 43  \n",
      " revol_bal       | 30  \n",
      " revol_util      | 41  \n",
      " total_acc       | 36  \n",
      " policy_code     | 7   \n",
      " acc_now_delinq  | 20  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "for i in double_cols:\n",
    "    df = df.withColumn(i, F.col(i).try_cast('double'))\n",
    "\n",
    "# Count the number of NaNs.\n",
    "df.select([\n",
    "    F.count(\n",
    "        F.when(\n",
    "            F.col(c).isNull(), 1)\n",
    "    ).alias(c)\n",
    "    for c in double_cols\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33534410-b155-4317-a2dc-585a65bfb5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72500a3-eed5-497c-84e7-c6bb61bbcdfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### EDA Summary\n",
    "\n",
    "- Column types have been analyzed\n",
    "- Columns with more than 10% missing values have been removed\n",
    "- Post-issue columns have been excluded to prevent data leakage\n",
    "- Numerical columns have been cast to type `double`\n",
    "- Remaining rows with NaN values (<1% of the data) have been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ff0c9-4c4e-4976-9a8c-acdda0add7ca",
   "metadata": {},
   "source": [
    "## Feature Engineering | String type to numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21cf7d2-3366-402a-8f9a-873f9dfba354",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Inspecting String Columns\n",
    "\n",
    "First, we identify all columns with string data types to inspect their contents and understand categorical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "852c9b4d-fd7e-47c1-af74-7f479285610b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loan_amnt', 'double'),\n",
       " ('funded_amnt', 'double'),\n",
       " ('funded_amnt_inv', 'double'),\n",
       " ('term', 'string'),\n",
       " ('int_rate', 'double'),\n",
       " ('installment', 'double'),\n",
       " ('grade', 'string'),\n",
       " ('sub_grade', 'string'),\n",
       " ('emp_length', 'string'),\n",
       " ('home_ownership', 'string'),\n",
       " ('annual_inc', 'double'),\n",
       " ('verification_status', 'string'),\n",
       " ('loan_status', 'string'),\n",
       " ('purpose', 'string'),\n",
       " ('title', 'string'),\n",
       " ('addr_state', 'string'),\n",
       " ('dti', 'double'),\n",
       " ('delinq_2yrs', 'double'),\n",
       " ('earliest_cr_line', 'string'),\n",
       " ('inq_last_6mths', 'double'),\n",
       " ('open_acc', 'double'),\n",
       " ('pub_rec', 'double'),\n",
       " ('revol_bal', 'double'),\n",
       " ('revol_util', 'double'),\n",
       " ('total_acc', 'double'),\n",
       " ('policy_code', 'double'),\n",
       " ('application_type', 'string'),\n",
       " ('acc_now_delinq', 'double')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23e8bafc-8c00-4262-9813-4a354597b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_cols = []\n",
    "for i in df.dtypes:\n",
    "    if i[1] == 'string': str_cols+=[i[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5be0cc57-e9a1-4821-b4e8-0fbb172c15fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " term                |  36 months  \n",
      " grade               | B           \n",
      " sub_grade           | B2          \n",
      " emp_length          | 10+ years   \n",
      " home_ownership      | RENT        \n",
      " verification_status | Verified    \n",
      " loan_status         | Fully Paid  \n",
      " purpose             | credit_card \n",
      " title               | Computer    \n",
      " addr_state          | AZ          \n",
      " earliest_cr_line    | Jan-1985    \n",
      " application_type    | INDIVIDUAL  \n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "df.select(*str_cols).show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722962c-bf7e-484d-a5f2-3c9b3f14e21e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preparing Data for Gradient Boosted Trees (GBT)\n",
    "\n",
    "We outline the transformations needed for each feature to make the dataset suitable for GBT:\n",
    "\n",
    "- `emp_length`: ordinal encoding\n",
    "- `term`: ordinal encoding\n",
    "- `grade` + `sub_grade`: ordinal encoding (only `sub_grade` will be used)\n",
    "- `home_ownership`: one-hot encoding\n",
    "- `verification_status`: ordinal encoding\n",
    "- `purpose`: one-hot encoding\n",
    "- `title`: drop (too many unique values)\n",
    "- `addr_state`: map to regions (West/East/Midwest/South) → one-hot encoding\n",
    "- `earliest_cr_line`: convert to years as integer\n",
    "- `application_type`: drop (only one unique value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7c89a-c7f9-4647-b7a0-9b6ad87dda56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ordinal Encoding: emp_length\n",
    "\n",
    "We map employment length to ordinal values and fill missing values (`n/a`) with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e545208e-db25-4cc3-994d-c47fb507c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_length_mapping = {\n",
    "    \"< 1 year\": 0,\n",
    "    \"1 year\": 1,\n",
    "    \"2 years\": 2,\n",
    "    \"3 years\": 3,\n",
    "    \"4 years\": 4,\n",
    "    \"5 years\": 5,\n",
    "    \"6 years\": 6,\n",
    "    \"7 years\": 7,\n",
    "    \"8 years\": 8,\n",
    "    \"9 years\": 9,\n",
    "    \"10+ years\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10415b3d-1e11-427b-85c0-1630f9a2fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_length_mapping.items()\n",
    "emp_map = []\n",
    "for i in emp_length_mapping.items(): emp_map += i\n",
    "mapping_expr = F.create_map([F.lit(x) for x in emp_map])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3a97036-4d7d-4497-9fdc-918c196f2235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    'emp_length',\n",
    "    mapping_expr[F.col('emp_length')]\n",
    ")\n",
    "df = df.fillna(df.agg(F.median(F.col('emp_length'))).collect()[0][0]) # we have n/a values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d78b8-a92c-416e-9bc9-39f72c1eaf71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ordinal Encoding: term\n",
    "\n",
    "We remove the \" months\" text and cast the column to numeric type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66ddf12a-081c-4cdf-bc67-0669c1c5f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    'term',\n",
    "    F.regexp_replace(F.col('term'),' months','').try_cast('double')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb40bcf-3c42-4b16-8d38-61402a088e44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ordinal Encoding: sub_grade + grade\n",
    "\n",
    "We convert sub_grade values into an ordinal numeric scale.  \n",
    "Grades will be mapped as follows: E5=1, E4=2, …, A1=35.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40bb78b1-810f-463f-aa0e-d031e2531323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grade\n",
    "count_arr = []\n",
    "c = 1\n",
    "for i in ['G','F','E','D','C','B','A']:\n",
    "    for j in range(1,5+1):\n",
    "        count_arr+=[f'{i}{j}']+[c]\n",
    "        c+=1\n",
    "grade_expr = F.create_map([F.lit(i) for i in count_arr])\n",
    "\n",
    "df = df.withColumn(\n",
    "    'un_grade',\n",
    "    grade_expr[F.col('sub_grade')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df35ed-146d-47a6-9e8d-01f14334cc22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ordinal Encoding: verification_status\n",
    "\n",
    "Mapping verification status to ordinal numeric values for model compatibility:\n",
    "- 'Not Verified' → 1\n",
    "- 'Source Verified' → 2\n",
    "- 'Verified' → 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88ecbcce-47bd-4a5c-98e3-9b7e8fce1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verification_status\n",
    "verification_map = ['Not Verified',1,'Source Verified',2,'Verified',3]\n",
    "verification_expr = F.create_map([F.lit(i) for i in verification_map]) \n",
    "df = df.withColumn(\n",
    "    'verification_status',\n",
    "    verification_expr[F.col('verification_status')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71387612-fa47-4b12-9383-32cac3b89620",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### One-Hot Encoding: home_ownership and purpose\n",
    "\n",
    "We convert categorical string variables into numeric vectors using StringIndexer + OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f71ba038-3a07-49b8-ae8b-da8f52527eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# home_ownership\n",
    "indexer = StringIndexer(inputCol='home_ownership',outputCol='home_ownership_ind')\n",
    "df = indexer.fit(df).transform(df)\n",
    "encoder = OneHotEncoder(inputCol='home_ownership_ind',outputCol='home_ownership_ohe')\n",
    "df = encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af78a085-6564-4cd8-a3a2-e6d71d20746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# purpose\n",
    "indexer = StringIndexer(inputCol='purpose',outputCol='purpose_ind')\n",
    "df = indexer.fit(df).transform(df)\n",
    "encoder = OneHotEncoder(inputCol='purpose_ind',outputCol='purpose_ohe')\n",
    "df = encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb6897-7526-40a5-a7b4-2a14bb62add3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Date Transformation: earliest_cr_line\n",
    "\n",
    "We convert the earliest credit line to a date format and calculate the number of years since the credit line was opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c02b09bc-bec8-464b-ae74-5e033b8ad148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# earliest_cr_line\n",
    "df = df.withColumn(\n",
    "    \"earliest_cr_line_date\",\n",
    "    F.to_date(F.concat(F.lit(\"01-\"), F.col(\"earliest_cr_line\")), \"dd-MMM-yyyy\")\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"years_since\",\n",
    "    F.round(F.months_between(F.current_date(), \"earliest_cr_line_date\") / 12, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce543e9-34ac-4b40-95ef-f53f6c5e9536",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Mapping States to Regions\n",
    "\n",
    "We group US states into regions: West, East, Midwest, South, and Other.  \n",
    "Then we apply one-hot encoding to the `region` feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "725c08b2-3461-49d1-8de0-7b2bb65b7dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# region\n",
    "df = df.withColumn(\n",
    "    \"region\",\n",
    "    F.when(F.col(\"addr_state\").isin(\n",
    "        \"WA\",\"OR\",\"CA\",\"NV\",\"ID\",\"MT\",\"WY\",\"UT\",\"CO\",\"AK\",\"HI\",'AZ'), \"West\")\n",
    "     .when(F.col(\"addr_state\").isin(\n",
    "        \"NY\",\"NJ\",\"PA\",\"MA\",\"CT\",\"RI\",\"NH\",\"VT\",\"ME\",\"DC\",\"MD\",\"DE\"), \"East\")\n",
    "     .when(F.col(\"addr_state\").isin(\n",
    "        \"IL\",\"IN\",\"MI\",\"OH\",\"WI\",\"IA\",\"KS\",\"MN\",\"MO\",\"NE\",\"ND\",\"SD\"), \"Midwest\")\n",
    "     .when(F.col(\"addr_state\").isin(\n",
    "        \"TX\",\"FL\",\"GA\",\"AL\",\"MS\",\"LA\",\"OK\",\"AR\",\"TN\",\"SC\",\"NC\",\"KY\",\"VA\",\"WV\",\"NM\"), \"South\")\n",
    "     .otherwise(\"Other\")\n",
    ")\n",
    "indexer = StringIndexer(inputCol='region',outputCol='region_ind')\n",
    "df = indexer.fit(df).transform(df)\n",
    "encoder = OneHotEncoder(inputCol='region_ind',outputCol='region_ohe')\n",
    "df = encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2503ec21-a15e-475d-a29c-01974e889b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(*['sub_grade','grade','title','application_type','home_ownership','purpose',\n",
    "               'earliest_cr_line','earliest_cr_line_date','region','purpose_ind',\n",
    "               'home_ownership_ind','addr_state','region_ind'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b661443-dbce-4fd1-98f5-6181738551f6",
   "metadata": {},
   "source": [
    "### Target Variable: loan_status\n",
    "\n",
    "We will use a **binary classification** for `loan_status` to prevent data leakage.  \n",
    "Instead of a ternary classification, we focus on distinguishing between two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "211ee9e8-117c-4b6c-acd5-d095b6c7ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|loan_status_bin|count|\n",
      "+---------------+-----+\n",
      "|              1|76630|\n",
      "|              0|11419|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with 'Does not meet the criteria'\n",
    "df = df.filter(~F.col('loan_status').startswith(\"Does not meet the\"))\n",
    "\n",
    "# Create binary target: 1 = paid/current, 0 = default/late\n",
    "df = df.withColumn(\n",
    "    'loan_status_bin',\n",
    "    F.when(F.col('loan_status').isin('Fully Paid', 'Current'), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Check distribution\n",
    "df.groupBy('loan_status_bin').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea8c4d2f-5b82-423a-840c-23616d83e1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/21 22:09:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/12/21 22:09:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/12/21 22:09:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/12/21 22:09:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/12/21 22:09:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/12/21 22:09:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "25/12/21 22:09:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "25/12/21 22:10:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "25/12/21 22:10:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/12/21 22:10:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/12/21 22:10:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/12/21 22:10:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/12/21 22:10:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .parquet(\"../data/loans_ml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
